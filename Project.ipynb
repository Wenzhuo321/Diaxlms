{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d9cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7284bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language                                           Question  \\\n",
      "0  English      What does “þeod-cyninga, þrym gefrunon” mean？   \n",
      "1  English  What does “egsode eorl. Syððan ærest wearð ” m...   \n",
      "2  English                         What does “Sōþlīċe ” mean？   \n",
      "3  English  And ic cyðe eow, þæt ic wylle beon hold hlafor...   \n",
      "4  English            What does “ \\tTōbecume þīn rīċe,” mean？   \n",
      "\n",
      "                                     Standard Answer Period  \n",
      "0  [of] people-kings, trim (glory) afrained (have...    NaN  \n",
      "1  awed earls (leaders of men). Sith (since) erst...    NaN  \n",
      "2                                            Soothly    NaN  \n",
      "3  And I kithe(make known/couth to) you, that I w...    NaN  \n",
      "4                          Come thy riche (kingdom),    NaN  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"/Users/wenzhuochen/Desktop/diaxlms/term paper/Dataset.xlsx\")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c5f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df[['Language', 'Question','Standard Answer']]\n",
    "\n",
    "# Specify the file path for the output JSON file\n",
    "json_file_path = \"/Users/wenzhuochen/Desktop/diaxlms/term paper/Dataset.json\"\n",
    "\n",
    "# Convert the DataFrame to a JSON file\n",
    "df_selected.to_json(json_file_path, orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e04d6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('dataset.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON from line: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2c6e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = []\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ed7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    # Check if 'Question' exists and is not None\n",
    "    if 'Question' in item and item['Question'] is not None:\n",
    "        response = openai.ChatCompletion.create(\n",
    "          model=\"gpt-3.5-turbo\",\n",
    "          messages=[\n",
    "                {\"role\": \"user\", \"content\": item[\"Question\"]}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Assuming response structure is correct and response is successful\n",
    "        item[\"Generated Answer\"] = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        # Handle the case where 'Question' is missing or null\n",
    "        item[\"Generated Answer\"] = \"Question not provided or is null\"\n",
    "\n",
    "    generated_data.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16304752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(generated_data)\n",
    "\n",
    "csv_file_path = 'generated_data.csv'  \n",
    "df.to_csv('/Users/wenzhuochen/Desktop/diaxlms/term paper/generated_data.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a764e8c0",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c85bb540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language  F1 Score  Accuracy\n",
      "0  English       0.0       0.0\n",
      "1  Chinese       0.0       0.0\n",
      "2    Spain       0.0       0.0\n",
      "3   German       0.0       0.0\n",
      "4   French       0.0       0.0\n"
     ]
    }
   ],
   "source": [
    "def is_correct_answer(standard, generated):\n",
    "    return int(standard == generated)\n",
    "\n",
    "# 应用这个函数来创建一个新的列'is_correct'\n",
    "df['is_correct'] = df.apply(lambda row: is_correct_answer(row['Standard Answer'], row['Generated Answer']), axis=1)\n",
    "\n",
    "# 计算每种语言的F1分数和准确率\n",
    "languages = df['Language'].unique()  # 获取所有语言\n",
    "results = []\n",
    "\n",
    "for lang in languages:\n",
    "    lang_df = df[df['Language'] == lang]\n",
    "    true_answers = [1] * len(lang_df)  # 假设所有标准答案都是正确的，即值为1\n",
    "    predicted_answers = lang_df['is_correct'].tolist()\n",
    "    \n",
    "    # 计算F1分数和准确率\n",
    "    f1 = f1_score(true_answers, predicted_answers, average='binary')\n",
    "    accuracy = accuracy_score(true_answers, predicted_answers)\n",
    "    \n",
    "    results.append({'Language': lang, 'F1 Score': f1, 'Accuracy': accuracy})\n",
    "\n",
    "# 将结果转换为DataFrame以便更好地展示\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40bec3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41ed41f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "BLEU Score: 0.022911398113104053\n",
      "ROUGE-1 Score: 0.1857605448110485\n",
      "ROUGE-2 Score: 0.07763845702503278\n",
      "ROUGE-L Score: 0.17212156540506013\n",
      "\n",
      "Chinese:\n",
      "BLEU Score: 0.0\n",
      "ROUGE-1 Score: 0.0\n",
      "ROUGE-2 Score: 0.0\n",
      "ROUGE-L Score: 0.0\n",
      "\n",
      "Spain:\n",
      "BLEU Score: 0.02587884481055338\n",
      "ROUGE-1 Score: 0.1808176381089021\n",
      "ROUGE-2 Score: 0.07340191445358295\n",
      "ROUGE-L Score: 0.17209827051996934\n",
      "\n",
      "German:\n",
      "BLEU Score: 0.013589836454858431\n",
      "ROUGE-1 Score: 0.18314557497345046\n",
      "ROUGE-2 Score: 0.09086289586335192\n",
      "ROUGE-L Score: 0.18314557497345046\n",
      "\n",
      "French:\n",
      "BLEU Score: 0.015614651403209511\n",
      "ROUGE-1 Score: 0.18965912789103342\n",
      "ROUGE-2 Score: 0.08432634915297174\n",
      "ROUGE-L Score: 0.1801748159929284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 初始化rouge计算工具\n",
    "rouge = Rouge()\n",
    "\n",
    "# 准备存储结果\n",
    "results = {}\n",
    "\n",
    "# 对每种语言单独处理\n",
    "for language in df['Language'].unique():\n",
    "    language_data = df[df['Language'] == language]\n",
    "    bleu_scores = []\n",
    "    rouge_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}\n",
    "    \n",
    "    for index, row in language_data.iterrows():\n",
    "        standard_answer = row['Standard Answer'].split()\n",
    "        generated_answer = row['Generated Answer'].split()\n",
    "        \n",
    "        # 计算BLEU分数\n",
    "        bleu_score = sentence_bleu([standard_answer], generated_answer, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        bleu_scores.append(bleu_score)\n",
    "        \n",
    "        # 计算ROUGE分数\n",
    "        scores = rouge.get_scores(' '.join(generated_answer), ' '.join(standard_answer))\n",
    "        for key in rouge_scores.keys():\n",
    "            rouge_scores[key].append(scores[0][key]['f'])\n",
    "            \n",
    "    # 计算平均分数\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    avg_rouge = {key: np.mean(values) for key, values in rouge_scores.items()}\n",
    "    \n",
    "    # 存储结果\n",
    "    results[language] = {'BLEU': avg_bleu, **avg_rouge}\n",
    "\n",
    "# 打印结果\n",
    "for language, scores in results.items():\n",
    "    print(f\"{language}:\\nBLEU Score: {scores['BLEU']}\")\n",
    "    for key, value in scores.items():\n",
    "        if key != 'BLEU':\n",
    "            print(f\"{key.upper()} Score: {value}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d70cf3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7877d62ee848a9a57abb60d2aaa853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4530cec60d346b183300a220c25f1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadf687b9d6f421ea584173100ea00d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e957d44f234c8980cef58e92e024d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9c6431543140e1831b747000f207bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c62bfb198f4a7dac8697c3aa54cf10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.05 seconds, 19.03 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f44d12b60a4463b8772191be215f8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b2e484b0134a3283635d2e0f5339bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.97 seconds, 10.16 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1a92f9be93485f9651377b43b86d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00c6ac14e2b47c99605fdfd759e6b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.95 seconds, 10.25 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59555a680904ac58228ad6517a5b9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18136c655e54c8dbe06734a668cd7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.07 seconds, 18.73 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a4f1d1378b45979a348757d6030e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43c27709e5145df865ebf5aca868949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.81 seconds, 11.07 sentences/sec\n",
      "English - Precision: 0.6320205926895142, Recall: 0.7414064407348633, F1: 0.6805657744407654\n",
      "Chinese - Precision: 0.6859683394432068, Recall: 0.7482554912567139, F1: 0.7153390645980835\n",
      "Spain - Precision: 0.6370960474014282, Recall: 0.7291144132614136, F1: 0.6790500283241272\n",
      "German - Precision: 0.609628438949585, Recall: 0.7574683427810669, F1: 0.6747456789016724\n",
      "French - Precision: 0.6582952737808228, Recall: 0.7454506158828735, F1: 0.6979616284370422\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bert_score import score\n",
    "\n",
    "\n",
    "# 结果字典\n",
    "results = {}\n",
    "\n",
    "# 对每种语言单独处理\n",
    "for language in df['Language'].unique():\n",
    "    language_data = df[df['Language'] == language]\n",
    "    cands = language_data['Generated Answer'].tolist()\n",
    "    refs = language_data['Standard Answer'].tolist()\n",
    "    \n",
    "    # 计算BERTScore\n",
    "    P, R, F1 = score(cands, refs, lang=language, verbose=True)\n",
    "    \n",
    "    # 存储平均分数\n",
    "    results[language] = {\n",
    "        'Precision': P.mean().item(),\n",
    "        'Recall': R.mean().item(),\n",
    "        'F1': F1.mean().item()\n",
    "    }\n",
    "\n",
    "# 打印结果\n",
    "for language, scores in results.items():\n",
    "    print(f\"{language} - Precision: {scores['Precision']}, Recall: {scores['Recall']}, F1: {scores['F1']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec3341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
